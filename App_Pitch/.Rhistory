mtcars[, !(names(mtcars) %in% drops)]
mtcars <- mtcars[, !(names(mtcars) %in% drops)]
View(mtcars)
?shuttle
library(MASS)
?shuttle
View(shuttle)
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels =c(1,0))
View(shuttle)
exp(-0.2513)
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
fit1 <- glm(use ~ wind - 1, data = shuttle, family = "binomial")
View(shuttle)
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
fit1 <- glm(use ~ wind - 1, data = shuttle, family = "binomial")
summary(fit1)
windhead <- fit1$coef[1]
windtail <- fit1$coef[2]
exp(windtail)/exp(windhead)
View(shuttle)
fit<-glm(1 - use ~ factor(wind), family = binomial, data = shuttle)
View(shuttle)
data(shuttle)
fit<-glm(1 - use ~ factor(wind), family = binomial, data = shuttle)
View(shuttle)
shuttle$auto <- as.numeric(shuttle$use=="auto")
View(shuttle)
shuttle$auto <- as.numeric(shuttle$use=="auto")
fit <- glm(auto ~ wind,  binomial,  shuttle)
fit3 <- glm(1-auto ~ wind,  binomial, shuttle)
data(mtcars)
View(mtcars)
```{r}
#load the data
data(mtcars)
df <- mtcars
colnames(df) <- c("mpg", "cyl", "disp", "hp", "drat", "wt", "qsec", "engineType", "transmission", "gear", "carb")
#replace 0/1 in am to auto/manual
mtcars$am[mtcars$am == 1] <- " Manual"
mtcars$am[mtcars$am == 0] <- " Auto"
#replace 0/1 in vs to V-Engine/Straight-Engine
mtcars$vs[mtcars$vs == 1] <- " V-Engine"
mtcars$vs[mtcars$vs == 0] <- " Str-Engine"
#changing the names of columns to make it easier to understand
colnames(mtcars) <- c("mpg", "cyl", "disp", "hp", "drat", "wt", "qsec", "engineType", "transmission", "gear", "carb")
#change those numeric category variables to factors
mtcars$cyl <- as.factor(mtcars$cyl)
mtcars$gear <- as.factor(mtcars$gear)
mtcars$carb <- as.factor(mtcars$carb)
library(ggplot2)
g <- ggplot(mtcars, aes(transmission, mpg, fill = transmission))
g <- g + geom_boxplot(outlier.colour = "salmon", outlier.size = 3) + geom_jitter()
g
```
View(mtcars)
```{r, warning = FALSE}
```{r, warning = FALSE}
#load the data
data(mtcars)
df <- mtcars
colnames(df) <- c("mpg", "cyl", "disp", "hp", "drat", "wt", "qsec", "engineType", "transmission", "gear", "carb")
#replace 0/1 in am to auto/manual
mtcars$am[mtcars$am == 1] <- " Manual"
mtcars$am[mtcars$am == 0] <- " Auto"
#replace 0/1 in vs to V-Engine/Straight-Engine
mtcars$vs[mtcars$vs == 1] <- " V-Engine"
mtcars$vs[mtcars$vs == 0] <- " Str-Engine"
#changing the names of columns to make it easier to understand
colnames(mtcars) <- c("mpg", "cyl", "disp", "hp", "drat", "wt", "qsec", "engineType", "transmission", "gear", "carb")
#change those numeric category variables to factors
mtcars$cyl <- as.factor(mtcars$cyl)
mtcars$gear <- as.factor(mtcars$gear)
mtcars$carb <- as.factor(mtcars$carb)
library(ggplot2)
g <- ggplot(mtcars, aes(transmission, mpg, fill = transmission))
g <- g + geom_boxplot(outlier.colour = "salmon", outlier.size = 3) + geom_jitter()
g
```
View(mtcars)
?mtcars
data(mtcars)
summary(lm(mpg ~. , data = mtcars))
summary(lm(mpg ~ factor(cyl) + disp + hp + factor(drat) + wt + qsec + factor(vs) + factor(am) + factor(gear) + factor(carb) , data = mtcars))
summary(lm(mpg ~ factor(cyl) + disp + hp + wt + qsec + factor(vs) + factor(am) + factor(gear) + factor(carb) , data = mtcars))
summary(lm(mpg ~ factor(cyl) + disp + hp + wt + qsec + factor(vs) + factor(am) + factor(gear) + factor(carb) -1 , data = mtcars))
fitConsole <- (lm(mpg ~ factor(cyl) + disp + hp + wt + qsec + factor(vs) + factor(am) + factor(gear) + factor(carb), data = mtcars))
fitConsole$residuals
fitConsole$residuals[1]
adj.r.squared(fitConsole)
sumTrans <- summary(lm(mpg ~ factor(cyl) + disp + hp + wt + qsec + factor(vs) + factor(am) + factor(gear) + factor(carb) -1 , data = mtcars))
sumTrans$adj.r.squared
sumTrans$residuals
sumTrans$sigma
sumTrans
fitconsole
fitConsole
summary(fitConsolele)
summary(fitConsole)
fit <- lm(formula = mpg ~ factor(cyl) + disp + hp + wt + qsec + factor(vs) + factor(am) + factor(gear) + factor(carb) + drat, data = mtcars)
summary(fit)
fit <- lm(formula = mpg ~ factor(cyl) + disp + hp + wt + qsec + factor(vs) + factor(am) + factor(gear) + factor(carb) + drat - 1, data = mtcars)
summary(fot)
summary(fit)
23.87913 + 1.21212
fit <- lm(formula = mpg ~ factor(cyl) + disp + hp + wt + qsec + factor(vs) + factor(am) + factor(gear) + factor(carb) + drat, data = mtcars)
summary(fit)
fit <- lm(formula = mpg ~ factor(cyl) + disp + hp + wt + qsec + factor(vs) + factor(am) - 1 + factor(gear) + factor(carb) + drat, data = mtcars)
summary(fit)
(0.99*0.001)/((0.99*0.001)+0.01*0.999)
((0.99*0.001)/((0.99*0.001)+0.01*0.999))*100
'Week 3 Question 1'
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
'subset to training and testing set'
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
predict(modFit, newdata = train)
library(rpart)
set.seed(125)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
data(segmentationOriginal)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
set.seed(125)
'subset to training and testing set'
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
predict(modFit, newdata = train)
install.packages("rattle")
modFit <- train(Class ~ ., data = train, method = "rpart")
install.packages("e1071")
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
library("rattle", lib.loc="~/R/win-library/3.1")
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
'Week 3 Question 1'
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
set.seed(125)
'subset to training and testing set'
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
predict(modFit, newdata = train)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training = segmentationOriginal[segmentationOriginal$Case == "Train",]
testing = segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
M <- train(Class ~ ., data=training, method="rpart")
M
M$finalModel
plot(M$finalModel)
text(M$finalModel)
library(rattle)
library(rattle)
fancyRpartPlot(M$finalModel)
install.packages("rpart.plot")
library(rattle)
fancyRpartPlot(M$finalModel)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
M <- train(Area ~ ., data=olive, method="rpart")
newdata = as.data.frame(t(colMeans(olive)))
newdata
predict(M, newdata)
M$finalModel
predict(M, newdata)
fancyRpartPlot(M$finalModel)
View(newdata)
View(olive)
View(newdata)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
M <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method="glm", family="binomial")
M
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, predict(M, testSA))
missClass(trainSA$chd, predict(M, trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
M <- train(y ~ ., data=vowel.train, method="rf")
varImp(M)
'Week 3 Question 1'
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training = segmentationOriginal[segmentationOriginal$Case == "Train",]
testing = segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
M <- train(Class ~ ., data=training, method="rpart")
M
M$finalModel
plot(M$finalModel)
text(M$finalModel)
library(rattle)
fancyRpartPlot(M$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(M,predData)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train
View(vowel.train)
vowel.train$y = as.factor(vowel.train$y)
seed.set(33833)
set.seed(33833)
library(caret)
rfModel <- train(y ~., method = "rf", data = vowel.train)
gbmModel <- train(y~., method = "gmb", data = vowel.train)
gbmModel <- train(y~., method = "gbm", data = vowel.train)
rfModel
gbModel
gbmModel
predict1 <- predict(rfModel, newdata = vowel.test)
predict2 <- predict(gbmModel, newdata = vowel.test)
predict1
c1 <- confusionMatrix(predict1, vowel.test$y)
c1
c1$table
c2 <- confusionMatrix(predict2, vowel.test$y)
c2
c1$overall
c1$overall
c2$overall
predict1
predict1$y
predict1 == predict2
accuracy <- (predict1 == predict2)
accuracy[,TRUE]
dim(accuracy)
accuracy
as.dataframe(accuracy)
as.data.frame(accuracy)
accuracy[accuracy == TRUE,]
accuracy[accuracy == TRUE]
dim(accuracy[accuracy == TRUE])
?count
count(accuracy == TRUE)
314/462
c2$overall
count(accuracy)
314 / 462
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
table(vowel.train$y)
set.seed(33833)
require(caret)
M1 <- train(y ~ ., data=vowel.train, method="rf")
M2 <- train(y ~ ., data=vowel.train, method="gbm")
hat1 <- predict(M1, vowel.test)
hat2 <- predict(M2, vowel.test)
confusionMatrix(hat1, vowel.test$y)$overall
confusionMatrix(hat2, vowel.test$y)$overall
hat <- data.frame(hat1,
hat2,
y = vowel.test$y,
agree = hat1 == hat2)
accuracy <- sum(hat1[hat$agree] == hat$y[hat$agree]) / sum(hat$agree)
accuracy
seed.set(62433)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
?AppliedPredictiveModeling
?AlzheimerDisease
??AlzheimerDisease
data()
data(package = .packages(all.available = TRUE)
)
data(AlzheimerDisease)
data(twoClassData)
detach("package:AppliedPredictiveModeling", unload=TRUE)
library("AppliedPredictiveModeling", lib.loc="~/R/win-library/3.1")
data(AlzheimerDisease)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
M1 <- train(diagnosis ~ ., data=training, method="rf")
M2 <- train(diagnosis ~ ., data=training, method="gbm")
M3 <- train(diagnosis ~ ., data=training, method="lda")
hat1 <- predict(M1, testing)
hat2 <- predict(M2, testing)
hat3 <- predict(M3, testing)
hat <- data.frame(hat1, hat2, hat3, diagnosis=testing$diagnosis)
M4 <- train(diagnosis ~ ., data=hat, method="rf")
M4
hat4 <- predict(M4, testing)
confusionMatrix(hat1, testing$diagnosis)$overall
confusionMatrix(hat2, testing$diagnosis)$overall
confusionMatrix(hat3, testing$diagnosis)$overall
confusionMatrix(hat4, testing$diagnosis)$overall
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
fit1 <- train(diagnosis~., data = training, method ="rf")
fit2 <- train(diagnosis~., data = training, method ="gbm")
fit3 <- train(diagnosis~., data = training, method ="lda")
predict1 <- predict(fit1, testing)
predict2 <- predict(fit2, testing)
predict3 <- predict(fit3, testing)
predict1
combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis)
fit4 <- train(diagnosis~. , data = combined, method = "rf")
fit4 <- train(diagnosis~. , data = combined, method = "rf")
confusionMatrix(predict1, testing$diagnosis)$overall
predict4 <- predict(fit4, testing)
confusionMatrix(predict1, testing$diagnosis)$overall
confusionMatrix(predict2, testing$diagnosis)$overall
confusionMatrix(predict4, testing$diagnosis)$overall
confusionMatrix(predict4, testing$diagnosis)$overall
clear
clear()
confusionMatrix(predict1, testing$diagnosis)$overall
confusionMatrix(predict2, testing$diagnosis)$overall
confusionMatrix(predict3, testing$diagnosis)$overall
confusionMatrix(predict4, testing$diagnosis)$overall
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
M1 <- train(CompressiveStrength ~ ., data=training, method="lasso")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
M1 <- train(CompressiveStrength ~ ., data=training, method="lasso")
M1
plot(M1$finalModel, xvar="penalty")
?plot.enet
library(lubridate)  # For year() function below
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
dat = read.csv(url)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
require(forecast)
M <- bats(tstrain)
M
hat <- forecast(M, length(testing$visitsTumblr))
hat <- cbind(testing, data.frame(hat))
hat$isIn95 <- hat$Lo.95 < hat$visitsTumblr & hat$visitsTumblr < hat$Hi.95
prop.table(table(hat$isIn95))
install.packages("lubridate")
library(lubridate)  # For year() function below
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
dat = read.csv(url)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
require(forecast)
M <- bats(tstrain)
M
hat <- forecast(M, length(testing$visitsTumblr))
hat <- cbind(testing, data.frame(hat))
hat$isIn95 <- hat$Lo.95 < hat$visitsTumblr & hat$visitsTumblr < hat$Hi.95
prop.table(table(hat$isIn95))
install.packages("forecast")
library(lubridate)  # For year() function below
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
dat = read.csv(url)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
require(forecast)
M <- bats(tstrain)
M
hat <- forecast(M, length(testing$visitsTumblr))
hat <- cbind(testing, data.frame(hat))
hat$isIn95 <- hat$Lo.95 < hat$visitsTumblr & hat$visitsTumblr < hat$Hi.95
prop.table(table(hat$isIn95))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
require(e1071)
M <- svm(CompressiveStrength ~ ., data=training)
testing$hat <- predict(M, testing)
testing$error <- testing$CompressiveStrength - testing$hat
rmse <- sqrt(mean(testing$error ^ 2))
rmse
install.packages("devtools")
require(devtools)
install_github("slidify", "ramnathv")
library(slidify)
setwd("~/Desktop/9) Developing Data Products/Project/App_Pitch")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
browseURL("index.html")
